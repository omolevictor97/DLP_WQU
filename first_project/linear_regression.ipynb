{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, before we build powerful neural networks, we must first understand their most basic form: the **linear model**.  \n",
    "Think of this section as our launchpad ‚Äî- the point where we first define the idea of a \"model\" in machine learning.\n",
    "\n",
    "We'll explore:\n",
    "- What linear regression is,\n",
    "- How it relates to neural networks,\n",
    "- And how it applies to real-world problems like predicting concrete strength.\n",
    "\n",
    "#### **What is Linear Regression?**\n",
    "\n",
    "**Linear regression** is one of the simplest‚Äîand most important‚Äîtechniques in supervised learning.\n",
    "\n",
    "It models the relationship between **inputs** and **outputs** by assuming a **straight-line relationship** between them.\n",
    "\n",
    "Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ = Input features (tensor)  \n",
    "- $W$ = Weights (learnable parameters)  \n",
    "- $b$ = Bias term  \n",
    "- $\\hat{y}$ = Predicted output\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "> \"We‚Äôre trying to find the best straight line that explains the relationship between what we know ($X$) and what we want to predict ($\\hat{y}$).\"\n",
    "\n",
    "This is the starting point for all of deep learning‚Äîeven the most complex neural networks are built from stacks of linear transformations like this.\n",
    "\n",
    "#### **Connection to Neural Networks**\n",
    "\n",
    "What happens if we take this linear model and place it inside a neural network?\n",
    "\n",
    "Actually, a **neural network with no hidden layers and no activation functions** behaves **exactly like a linear regression model**.  \n",
    "It computes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "No non-linearity, no depth‚Äîjust a direct, weighted transformation of inputs.\n",
    "\n",
    "> **Key Insight**:  \n",
    "> Linear models are the *core units* of deep learning.  \n",
    "> What makes neural networks powerful is that we **stack** these units and introduce **non-linear functions** between them.\n",
    "\n",
    "In this notebook, we‚Äôll start from this fundamental building block and eventually expand toward more complex networks.\n",
    "\n",
    "#### **Flow of a Linear Model**\n",
    "\n",
    "**Input Features** ($X$)  \n",
    "‚¨áÔ∏è  \n",
    "**Weighted Sum** ($XW$)  \n",
    "‚¨áÔ∏è  \n",
    "**Add Bias** ($+b$)  \n",
    "‚¨áÔ∏è  \n",
    "**Prediction** ($\\hat{y}$)\n",
    "\n",
    "\n",
    "#### **Real-World Applications of Linear Regression**\n",
    "\n",
    "Linear regression is not just a teaching example‚Äîit powers real applications:\n",
    "\n",
    "- üè† **Predicting house prices** based on area, location, and age\n",
    "- üë®‚Äçüíº **Estimating salary** based on experience and education\n",
    "- üèóÔ∏è **Predicting concrete strength** based on material composition (our case study!)\n",
    "\n",
    "> **Reflect & Connect:**  \n",
    "> Think about other real-world examples where you expect a *linear relationship*‚Äîcan you name one?\n",
    "\n",
    "In the next section, we‚Äôll take this mathematical idea and express it as code using PyTorch tensors.\n",
    "\n",
    "Time to answer **Multiple Choice questions 1.2.1.1 & 1.2.1.2**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Formulation of a Linear Model\n",
    "\n",
    "Now that we understand the intuition behind linear regression, let‚Äôs look more closely at the **mathematical structure**.\n",
    "\n",
    "We know the prediction rule is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "But what do these symbols actually mean?\n",
    "\n",
    "#### **Breaking Down the Equation**\n",
    "\n",
    "| Symbol      | Meaning                      | Typical Shape        |\n",
    "|-------------|------------------------------|-----------------------|\n",
    "| $X$         | Input features (matrix)      | $(n, d)$ = n samples √ó d features |\n",
    "| $W$         | Weights (vector/matrix)      | $(d, 1)$              |\n",
    "| $b$         | Bias (scalar or vector)      | Scalar or $(n, 1)$    |\n",
    "| $\\hat{y}$   | Predicted outputs            | $(n, 1)$              |\n",
    "\n",
    "So this model performs a **linear transformation** of the inputs, followed by a **bias shift**.  \n",
    "The multiplication $XW$ gives us a **weighted sum of features**, and $b$ lets us adjust the output.\n",
    "\n",
    "- $XW$: **Weighted Sums** ‚Üí shape $(n \\times 1)$  \n",
    "\n",
    "#### **Example: Manual Calculation**\n",
    "\n",
    "Let‚Äôs compute a concrete example with:\n",
    "- 3 data points\n",
    "- 2 input features per point\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "1.5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "b = 2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b =\n",
    "\\begin{bmatrix}\n",
    "1 \\cdot 0.5 + 2 \\cdot 1.5 + 2 \\\\\n",
    "3 \\cdot 0.5 + 4 \\cdot 1.5 + 2 \\\\\n",
    "5 \\cdot 0.5 + 6 \\cdot 1.5 + 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 \\\\\n",
    "11 \\\\\n",
    "16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Reflection Prompt:**  \n",
    "> Why must $X$ and $W$ have these exact shapes?  \n",
    "> What would go wrong if $W$ had shape (1, d) instead of (d, 1)?\n",
    "\n",
    "This is a key insight when building models with PyTorch‚Äî**tensor shapes must match** for the operations to work!\n",
    "\n",
    "In the next section, we‚Äôll move from this math into actual code: we‚Äôll define a `predict()` function that performs this operation using PyTorch tensors.\n",
    "\n",
    "Time to answer **Multiple Choice questions 1.2.2.1**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict Function on a Tiny Dataset\n",
    "\n",
    "So far, we've defined our model mathematically as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Now it's time to **bring this equation to life** using PyTorch.\n",
    "\n",
    "To keep things intuitive, we‚Äôll start with a **tiny toy dataset** that‚Äôs easy to follow.  \n",
    "This will help us understand how predictions are made using matrix multiplication.\n",
    "\n",
    "We‚Äôll:\n",
    "1. Create a simple dataset `X` with a few input values  \n",
    "2. Manually define parameters `W` and `b`  \n",
    "3. Implement a `predict(X)` function  \n",
    "4. Compute predictions $\\hat{y}$ using our linear model\n",
    "\n",
    "#### **Before the Code: What's Happening?**\n",
    "\n",
    "Let‚Äôs summarize the steps:\n",
    "\n",
    "- Create:\n",
    "  - A 4-sample input tensor `X` (1D feature)\n",
    "  - A target output `Y` that follows a linear pattern\n",
    "- Initialize:\n",
    "  - A simple weight `W = 0.5`\n",
    "  - A bias `b = 0`\n",
    "- Define a function:\n",
    "  - `predict(X)` that returns $XW + b$\n",
    "- Print the predicted output\n",
    "\n",
    "This helps build intuition for how weights and bias **shape the model‚Äôs output**‚Äîand it sets the stage for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tiny toy dataset\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])  # Inputs\n",
    "Y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])  # Ground truth\n",
    "\n",
    "# Step 2: Manually initialized weight and bias\n",
    "W = torch.tensor([[0.5]], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Step 3: Define predict function (≈∑ = XW + b)\n",
    "def predict(X):\n",
    "    return torch.matmul(X, W) + b\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = predict(X)\n",
    "\n",
    "# Step 5: Show results\n",
    "print(\"Predictions:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What did we just see?\n",
    "\n",
    "The model applied a simple **linear transformation** to the inputs:\n",
    "- Multiply each input by 0.5\n",
    "- Add 0\n",
    "\n",
    "This produces outputs: `[0.5, 1.0, 1.5, 2.0]` which are **significantly lower than** the true targets: `[2.0, 4.0, 6.0, 8.0]`\n",
    "\n",
    "‚úÖ This highlights that our model parameters **are not yet optimal**.  \n",
    "Later, we‚Äôll learn how to automatically update `W` and `b` using gradient descent to **minimize the difference** between predicted and actual values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Code Task 1.2.3.1:  Linear Prediction with PyTorch\n",
    "\n",
    "You are given a small dataset of input-output pairs:\n",
    "\n",
    "- Inputs: $X = [[1.0], [2.0], [3.0], [4.0]]$  \n",
    "- Targets: $Y = [[2.0], [4.0], [6.0], [8.0]]$\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Manually initialize:\n",
    "   - A weight tensor $W = [[0.5]]$ with `requires_grad=True`  \n",
    "   - A bias tensor $b = [0.0]$ with `requires_grad=True`\n",
    "2. Define a function `predict(X)` that returns $XW + b$\n",
    "3. Compute the predicted output: $\\hat{y} = \\text{predict}(X)$ and assign it to the variable `y_pred`\n",
    "4. Print the result\n",
    "\n",
    "Make sure to assign all values to the correct variable names as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Toy dataset\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "Y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Step 2: Manually initialized parameters\n",
    "W = torch.tensor([\n",
    "    [0.5]\n",
    "], requires_grad = True)\n",
    "b = torch.tensor([0.0], requires_grad = True)\n",
    "\n",
    "# Step 3: Define the predict function\n",
    "def predict(X):\n",
    "    return torch.matmul(X, W) + b\n",
    "\n",
    "# Step 4: Compute predictions\n",
    "y_pred = predict(X)\n",
    "\n",
    "# Step 5: Print predictions\n",
    "print(\"Predictions:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model makes predictions, we need a way to **measure how well it's doing**.  \n",
    "That‚Äôs where a **loss function** comes in.\n",
    "\n",
    "In supervised learning, a loss function compares the model's predictions $\\hat{y}$ to the true target values $y$ and returns a single number‚Äîa **score** that tells us how far off we are.\n",
    "\n",
    "> üéØ The goal of training is to **minimize this loss**.\n",
    "\n",
    "#### **Mean Squared Error (MSE)**\n",
    "\n",
    "For regression tasks, the most common loss function is the **Mean Squared Error (MSE)**.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = the actual value  \n",
    "- $\\hat{y}_i$ = the predicted value  \n",
    "- $n$ = number of examples\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "1. Take the **difference** between prediction and actual value  \n",
    "2. **Square** the difference (to penalize large errors more strongly)  \n",
    "3. **Average** across all data points\n",
    "\n",
    "This gives us a single loss value representing overall prediction error.\n",
    "\n",
    "#### **Code Preview: What Will We Do?**\n",
    "\n",
    "We‚Äôll:\n",
    "- Define a custom `mse_loss()` function in PyTorch\n",
    "- Pass in a small set of predictions and true values\n",
    "- Compute the MSE and print the result\n",
    "\n",
    "This is a good **sanity check** to verify our understanding of the loss calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.8750], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_loss(prediction, targets):\n",
    "    try:\n",
    "        if len(prediction) == len(targets):\n",
    "            squared_diff = sum((prediction - targets) ** 2)\n",
    "            return squared_diff / len(targets)\n",
    "    except Exception as e:\n",
    "        print(f\"prediction and target must be of same length\") \n",
    "        return None\n",
    "\n",
    "mse_loss(Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What Did We See?\n",
    "\n",
    "Our function computed the **average squared difference** between predicted and actual values.  \n",
    "This tells us how far off the model is‚Äî**lower MSE = better performance**.\n",
    "\n",
    "In our example, we get a small positive number showing modest prediction error.\n",
    "\n",
    "#### **Aside: Other Common Regression Losses**\n",
    "\n",
    "- **Mean Absolute Error (MAE):** Uses absolute differences; more robust to outliers  \n",
    "- **Huber Loss:** Combines MSE and MAE behavior; often used when we want stability with outliers\n",
    "\n",
    "\n",
    "In a upcoming section, we‚Äôll use this MSE function inside a **training loop** to guide the model in updating its parameters (œï).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Code Task 1.2.4.1: Compute Mean Squared Error (MSE) with 1D Vectors\n",
    "\n",
    "Implement and use a custom Mean Squared Error function using PyTorch.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Define a function named `mse_loss(predictions, targets)` that computes the MSE using the formula:  \n",
    "   $$\n",
    "   \\text{MSE} = \\frac{1}{n} \\sum (y_{\\text{true}} - y_{\\text{pred}})^2\n",
    "   $$\n",
    "\n",
    "2. Use the following tensors:  \n",
    "   - $Y_{\\text{true}} = [3.0, 5.0, 7.0, 9.0, 11.0]$  \n",
    "   - $Y_{\\text{pred}} = [2.5, 5.5, 6.0, 9.0, 12.0]$\n",
    "\n",
    "3. Compute the MSE and assign the result to a variable named `loss`.\n",
    "\n",
    "4. Print the result using `loss.item()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define MSE loss function\n",
    "def mse_loss(predictions, targets):\n",
    "    return torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Step 2: Define targets and predictions\n",
    "Y_true = torch.tensor([3.0, 5.0, 7.0, 9.0, 11.0])\n",
    "Y_pred = torch.tensor([2.5, 5.5, 6.0, 9.0, 12.0])\n",
    "\n",
    "# Step 3: Compute loss\n",
    "loss = mse_loss(Y_true, Y_pred)\n",
    "\n",
    "# Step 4: Print the result\n",
    "print(\"MSE Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing the Data and Model\n",
    "\n",
    "Now that we‚Äôve defined how the model works and how to measure its performance, it‚Äôs time to **prepare the data and the model parameters** for training.\n",
    "\n",
    "This is a critical setup step. A model with poor data preparation or unstable initialization will struggle to learn, even if the math is correct.\n",
    "\n",
    "We‚Äôll:\n",
    "1. Load the Concrete dataset\n",
    "2. Normalize the input features for stability\n",
    "3. Split the data into training and test sets\n",
    "4. Initialize the model parameters: weights `W` and bias `b`\n",
    "\n",
    "#### **Data & Model Setup: High-Level Plan**\n",
    "\n",
    "```text\n",
    "Load CSV\n",
    "    ‚Üì\n",
    "Extract inputs & targets\n",
    "    ‚Üì\n",
    "Standardize features (mean=0, std=1)\n",
    "    ‚Üì\n",
    "Split into 80% train, 20% test\n",
    "    ‚Üì\n",
    "Initialize W and b with gradient tracking\n",
    "```\n",
    "\n",
    "Let‚Äôs now implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([9, 1])\n",
      "torch.Size([824, 9]) torch.Size([824, 1])\n",
      "\n",
      "torch.Size([206, 9]) torch.Size([206, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# read data\n",
    "data = pd.read_csv('Project.csv')\n",
    "\n",
    "# separate features as needed\n",
    "inputs = data.iloc[:,:-1].values\n",
    "target = data.iloc[:,-1].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "input_scaled = scaler.fit_transform(inputs)\n",
    "\n",
    "# Convert both input and target features to tensor\n",
    "inputs_tensor = torch.tensor(input_scaled, dtype = torch.float32)\n",
    "target_tensor = torch.tensor(target, dtype = torch.float32)\n",
    "\n",
    "n_samples = len(inputs_tensor)\n",
    "indices = torch.randperm(n_samples)\n",
    "split_idx = int(n_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = inputs_tensor[train_indices], inputs_tensor[test_indices], target_tensor[train_indices], target_tensor[test_indices]\n",
    "\n",
    "\n",
    "# 6. Initialize model parameters\n",
    "num_features = X_train.shape[1] # number of feature\n",
    "W = torch.randn((num_features, 1), requires_grad=True)\n",
    "W.data *= 0.01 # scale down initial values\n",
    "b = torch.zeros((1,), requires_grad = True)\n",
    "\n",
    "print(b)\n",
    "print(W.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print()\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features vary widely in scale‚Äîe.g., cement can be over 500, while superplasticizer is often below 5.\n",
    "\n",
    "These differences lead to:\n",
    "- Large gradients\n",
    "- Exploding updates\n",
    "- Slow or unstable training\n",
    "\n",
    "‚úÖ Standardization ensures:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1  \n",
    "And helps the model **learn more reliably**.\n",
    "\n",
    "#### **Why Split the Dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how well the model **generalizes**, we set aside a **test set** that the model never sees during training.\n",
    "\n",
    "- **Training set**: used to update weights  \n",
    "- **Test set**: used to measure generalization\n",
    "\n",
    "We use a typical **80-20 split**.\n",
    "\n",
    "#### **Why Initialize with Small Weights?**\n",
    "\n",
    "If weights are too large at the beginning, the model might:\n",
    "- Produce massive outputs\n",
    "- Suffer from exploding gradients\n",
    "\n",
    "We initialize:\n",
    "- `W` as small random values (`~N(0, 0.01)`)\n",
    "- `b` as zero\n",
    "\n",
    "Also, by setting `requires_grad=True`, PyTorch will track gradients for these variables during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Parameters in PyTorch\n",
    "\n",
    "In the previous section, we initialized two model parameters: **weights** `W` and **bias** `b`.\n",
    "\n",
    "Let‚Äôs now take a closer look at what they represent, how they are shaped, and how PyTorch tracks their updates during training.\n",
    "\n",
    "#### **What Do the Parameters Represent?**\n",
    "\n",
    "- **Weights `W`**  \n",
    "  Each weight controls the influence of one input feature on the model‚Äôs output.\n",
    "  > ‚ÄúHow much does this feature matter in predicting concrete strength?‚Äù\n",
    "\n",
    "- **Bias `b`**  \n",
    "  A constant that shifts all predictions up or down‚Äîlike adjusting the baseline.\n",
    "\n",
    "Together, these parameters define the **linear function**:\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "#### **Understanding the Shapes**\n",
    "\n",
    "Since we have 8 input features, our parameter shapes must match the matrix multiplication rules:\n",
    "\n",
    "| Parameter | Shape     | Description                        |\n",
    "|-----------|-----------|------------------------------------|\n",
    "| `X`       | (n, 8)    | `n` samples, each with 8 features  |\n",
    "| `W`       | (8, 1)    | One weight per feature             |\n",
    "| `b`       | (1,)      | A single scalar bias               |\n",
    "| $\\hat{y}$ | (n, 1)    | One predicted value per example    |\n",
    "\n",
    "#### **Shape Flow Schematic**\n",
    "\n",
    "```text\n",
    "Input X:           (n √ó 8)\n",
    "Weights W:         (8 √ó 1)\n",
    "-------------------------\n",
    "XW result:       (n √ó 1)\n",
    "+ Bias b:        (broadcasted to n √ó 1)\n",
    "-------------------------\n",
    "Predictions ≈∑:   (n √ó 1)\n",
    "```\n",
    "‚úÖ PyTorch handles this **broadcasting automatically** when we add `b`.\n",
    "\n",
    "#### **Why `requires_grad=True`?**\n",
    "\n",
    "To train the model using **gradient descent**, PyTorch needs to compute the **gradients of the loss** with respect to the parameters:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi} = \\{W, b\\}\n",
    "$$\n",
    "\n",
    "We tell PyTorch which tensors to **track for gradients** by setting:\n",
    "```python\n",
    "W = torch.randn((8, 1), requires_grad=True)\n",
    "b = torch.zeros((1,), requires_grad=True)\n",
    "```\n",
    "\n",
    "This enables Automatic Differentiation in PyTorch: PyTorch builds a **computation graph** behind the scenes and stores all operations involving `W` and `b`.  \n",
    "When we later call `.backward()` on the loss, PyTorch uses the **chain rule** to compute:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi L\n",
    "$$\n",
    "\n",
    "This gives us the **gradient of the loss** with respect to each parameter.\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "- `W` and `b` are the **only parameters** in this model  \n",
    "- Their shapes must align with the **input-output structure**  \n",
    "- With `requires_grad=True`, PyTorch **tracks gradients** to help reduce the loss  \n",
    "\n",
    "In the next section, we‚Äôll introduce **gradient descent** ‚Äî the core algorithm that uses these gradients to **update `W` and `b`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient Descent ‚Äì Conceptual Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we saw that PyTorch can track how the loss depends on each parameter œï.  \n",
    "But knowing the gradient is only part of the story‚Äîwe still need to use that information to **actually improve the model**.\n",
    "\n",
    "That‚Äôs where **gradient descent** comes in.\n",
    "\n",
    "#### **The Goal: Minimize the Loss**\n",
    "\n",
    "Our model is only useful if it makes **accurate predictions**.  \n",
    "To measure how far off it is, we use a **loss function** $L[\\phi]$, which outputs a scalar value‚Äî  \n",
    "**the total error across the training data**.\n",
    "\n",
    "> üéØ Our goal is to **find the values of œï that make this loss as small as possible.**\n",
    "\n",
    "This is called an **optimization problem**.\n",
    "\n",
    "#### **The Role of the Gradient**\n",
    "\n",
    "The gradient is like a compass‚Äîit tells us:\n",
    "\n",
    "- üìâ **Which direction** will reduce the loss (the sign of the derivative)\n",
    "- üìè **How steeply** the loss changes (the magnitude)\n",
    "\n",
    "Each parameter has its own gradient:\n",
    "- $\\frac{\\partial L}{\\partial W}$ tells us how much to change the weights\n",
    "- $\\frac{\\partial L}{\\partial b}$ tells us how much to change the bias\n",
    "\n",
    "\n",
    "#### **The Gradient Descent Rule**\n",
    "\n",
    "We update the parameters using the following rule:\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\alpha \\cdot \\nabla_\\phi L\n",
    "$$\n",
    "\n",
    "Let‚Äôs unpack it:\n",
    "\n",
    "| Symbol         | Meaning                                |\n",
    "|----------------|----------------------------------------|\n",
    "| $\\phi$         | A model parameter (e.g., W or b)       |\n",
    "| $\\nabla_\\phi L$| Gradient of the loss wrt. that parameter |\n",
    "| $\\alpha$       | Learning rate (how big a step we take) |\n",
    "\n",
    "#### **Step-by-Step Update (Per Parameter)**\n",
    "\n",
    "```text\n",
    "1. Compute loss  L[œï]      ‚Üê How far off are our predictions?\n",
    "2. Compute gradient ‚àáœï L   ‚Üê How should we change W and b?\n",
    "3. Take a step:            ‚Üê Update each parameter:\n",
    "   œï ‚Üê œï - Œ± ‚ãÖ ‚àáœï L\n",
    "4. Repeat for many epochs\n",
    "```\n",
    "```text\n",
    "   X, y\n",
    "    ‚Üì\n",
    " forward pass: compute ≈∑ = XW + b\n",
    "    ‚Üì\n",
    "     loss = MSE(≈∑, y)\n",
    "    ‚Üì\n",
    " backward pass: compute gradients ‚àáW, ‚àáb\n",
    "    ‚Üì\n",
    " update: \n",
    "   W ‚Üê W - Œ± ‚ãÖ ‚àáW\n",
    "   b ‚Üê b - Œ± ‚ãÖ ‚àáb\n",
    "```\n",
    "\n",
    "#### **Choosing the Right Learning Rate**\n",
    "\n",
    "The learning rate $\\alpha$ controls **how fast** we move toward the minimum:\n",
    "\n",
    "| Learning Rate ($\\alpha$) | Behavior                      |\n",
    "|--------------------------|-------------------------------|\n",
    "| Too small                | Learns very slowly            |\n",
    "| Too large                | Overshoots or diverges        |\n",
    "| Just right               | Steady improvement in loss    |\n",
    "\n",
    "‚úÖ The **learning rate is a hyperparameter** ‚Äî we choose it **before training**.\n",
    "\n",
    "#### **Visual Intuition**\n",
    "\n",
    "Imagine you're standing on a **hilly surface**, trying to reach the **lowest point**:\n",
    "\n",
    "- The **gradient** tells you which way is downhill  \n",
    "- The **learning rate** tells you how big each step should be  \n",
    "\n",
    "üìå If steps are **too big**, you might **overshoot** or fall off a cliff  \n",
    "üìå If steps are **too small**, you'll take forever to reach the valley\n",
    "\n",
    "#### **Key Idea:**\n",
    "\n",
    "- Gradients are **directional signals**, not solutions  \n",
    "- We still need to decide **how to apply them** ‚Äî that's what **gradient descent** gives us\n",
    "\n",
    "‚úÖ **In PyTorch**:\n",
    "\n",
    "1. After computing the loss, we call `.backward()` to **populate gradients**  \n",
    "2. Then we **manually update the parameters** using these gradients inside a **training loop**\n",
    "\n",
    "Time to answer **Multiple Choice Questions 1.2.7.1 & 1.2.7.2**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop ‚Äì Manual Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the gradient descent update rule, it‚Äôs time to build a complete training loop and put all the moving parts together.\n",
    "\n",
    "This is where learning *actually happens*.  \n",
    "Our model starts with random weights and bias, and this loop will gradually adjust them to **minimize the loss** using the gradients.\n",
    "\n",
    "#### **What Does a Training Loop Do?**\n",
    "\n",
    "In every training **epoch** (iteration), we perform the following steps:\n",
    "\n",
    "1. **Forward pass** ‚Äì Compute predictions: $\\hat{y} = XW + b$  \n",
    "2. **Loss computation** ‚Äì Measure how far off the predictions are  \n",
    "3. **Backward pass** ‚Äì Compute gradients via `.backward()`  \n",
    "4. **Parameter update** ‚Äì Adjust `W` and `b` using gradient descent  \n",
    "5. **Gradient reset** ‚Äì Clear old gradients before the next epoch\n",
    "\n",
    "#### **Code block to be implemented later:**\n",
    "```python\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Predict\n",
    "    y_pred = predict(X_train)\n",
    "\n",
    "    # 2. Compute loss\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "\n",
    "    # 3. Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update weights\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # 5. Reset gradients\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "```\n",
    "#### **Why Use `torch.no_grad()`**\n",
    "\n",
    "When updating `W` and `b`, we **don‚Äôt want PyTorch to track** these operations in the computation graph.  \n",
    "Wrapping the update step in `torch.no_grad()` disables gradient tracking inside that block.\n",
    "\n",
    "#### **Why Reset Gradients?**\n",
    "\n",
    "By default, PyTorch **accumulates gradients** after each `.backward()` call.  \n",
    "So we must **reset them manually** using:\n",
    "\n",
    "```python\n",
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "```\n",
    "‚ùó If we don‚Äôt clear the gradients, PyTorch will add new gradients on top of the old ones, leading to incorrect updates during training.\n",
    "\n",
    "‚úÖ Let‚Äôs now implement the actual training loop below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1522.650146484375\n",
      "Epoch 20: Loss = 1465.5364990234375\n",
      "Epoch 30: Loss = 1410.7899169921875\n",
      "Epoch 40: Loss = 1358.30859375\n",
      "Epoch 50: Loss = 1307.9967041015625\n",
      "Epoch 60: Loss = 1259.761962890625\n",
      "Epoch 70: Loss = 1213.5162353515625\n",
      "Epoch 80: Loss = 1169.1754150390625\n",
      "Epoch 90: Loss = 1126.65869140625\n",
      "Epoch 100: Loss = 1085.88916015625\n"
     ]
    }
   ],
   "source": [
    "# set the learning rate and epochs\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# loop over the range pf epochs\n",
    "for epoch in range(epochs):\n",
    "    # make predictions\n",
    "    y_pred = predict(X_train)\n",
    "\n",
    "    # compute loss\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "\n",
    "    # make backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update model parameters\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    # zero grad\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    # print loss over 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üß† What Did We Just Build?\n",
    "\n",
    "This training loop applies **gradient descent manually** for 100 epochs.  \n",
    "Over time, the loss should go down, showing that our model is improving.\n",
    "\n",
    "‚úÖ By the end, our weight and bias should be better suited to the data  \n",
    "‚Äîmeaning: the model has **learned** from the input-output examples.\n",
    "\n",
    "Next, we‚Äôll visualize this improvement by plotting the **loss over epochs**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plotting Training Loss Over Epochs\n",
    "\n",
    "#### **Why Plot Training Loss?**\n",
    "\n",
    "During training, it's important to **monitor the loss over time** to understand how well the model is learning.\n",
    "\n",
    "#### **What Does the Loss Curve Tell Us?**\n",
    "\n",
    "- If the loss **decreases smoothly**, your model is learning.\n",
    "- If the loss **oscillates**, **increases**, or **stays flat**, it may indicate issues:\n",
    "  - Learning rate too high or too low\n",
    "  - Poor initialization\n",
    "  - Bugs in the training loop\n",
    "\n",
    "#### **Good vs. Bad Behavior**\n",
    "\n",
    "| Behavior                     | Interpretation                     |\n",
    "|-----------------------------|-------------------------------------|\n",
    "| Steadily decreasing loss    | ‚úÖ Model is learning properly        |\n",
    "| Increasing or unstable loss | ‚ö†Ô∏è Check learning rate or gradients |\n",
    "| Flat loss                   | ‚ö†Ô∏è Model may be stuck (not learning) |\n",
    "\n",
    "#### **Goal**\n",
    "\n",
    "We will:\n",
    "1. Track loss values during training\n",
    "2. Plot them after training using **matplotlib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1046.793212890625\n",
      "Epoch 20: Loss = 1009.3001098632812\n",
      "Epoch 30: Loss = 973.3428344726562\n",
      "Epoch 40: Loss = 938.8566284179688\n",
      "Epoch 50: Loss = 905.77978515625\n",
      "Epoch 60: Loss = 874.0534057617188\n",
      "Epoch 70: Loss = 843.620849609375\n",
      "Epoch 80: Loss = 814.4281616210938\n",
      "Epoch 90: Loss = 786.4236450195312\n",
      "Epoch 100: Loss = 759.5579223632812\n",
      "Epoch 110: Loss = 733.7833862304688\n",
      "Epoch 120: Loss = 709.0548095703125\n",
      "Epoch 130: Loss = 685.3287963867188\n",
      "Epoch 140: Loss = 662.563720703125\n",
      "Epoch 150: Loss = 640.7197875976562\n",
      "Epoch 160: Loss = 619.759033203125\n",
      "Epoch 170: Loss = 599.6448364257812\n",
      "Epoch 180: Loss = 580.34228515625\n",
      "Epoch 190: Loss = 561.8179321289062\n",
      "Epoch 200: Loss = 544.039794921875\n",
      "Epoch 210: Loss = 526.97705078125\n",
      "Epoch 220: Loss = 510.600341796875\n",
      "Epoch 230: Loss = 494.8816833496094\n",
      "Epoch 240: Loss = 479.79388427734375\n",
      "Epoch 250: Loss = 465.3112487792969\n",
      "Epoch 260: Loss = 451.408935546875\n",
      "Epoch 270: Loss = 438.06317138671875\n",
      "Epoch 280: Loss = 425.2513732910156\n",
      "Epoch 290: Loss = 412.9515686035156\n",
      "Epoch 300: Loss = 401.14300537109375\n",
      "Epoch 310: Loss = 389.8057861328125\n",
      "Epoch 320: Loss = 378.9205627441406\n",
      "Epoch 330: Loss = 368.4689636230469\n",
      "Epoch 340: Loss = 358.4334716796875\n",
      "Epoch 350: Loss = 348.7970886230469\n",
      "Epoch 360: Loss = 339.543701171875\n",
      "Epoch 370: Loss = 330.6578674316406\n",
      "Epoch 380: Loss = 322.1246643066406\n",
      "Epoch 390: Loss = 313.9297180175781\n",
      "Epoch 400: Loss = 306.0593566894531\n",
      "Epoch 410: Loss = 298.5005798339844\n",
      "Epoch 420: Loss = 291.24072265625\n",
      "Epoch 430: Loss = 284.2678527832031\n",
      "Epoch 440: Loss = 277.5703430175781\n",
      "Epoch 450: Loss = 271.13714599609375\n",
      "Epoch 460: Loss = 264.9576721191406\n",
      "Epoch 470: Loss = 259.0215148925781\n",
      "Epoch 480: Loss = 253.3191375732422\n",
      "Epoch 490: Loss = 247.84103393554688\n",
      "Epoch 500: Loss = 242.57815551757812\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "losses = []\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = predict(X_train)\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    # update weight and bias\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: Loss = {loss.item()}')\n",
    "\n",
    "plt.plot(np.arange(0, 500), losses, color='red', label=\"Epochs\", marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Training Loss Over Epoch')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ This plot helps us understand whether our training procedure is working and whether the learning rate and model parameters are reasonable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating Model Performance\n",
    "\n",
    "Training is complete‚Äîbut how do we know whether the model actually **learned something meaningful**?\n",
    "\n",
    "We now test our model on **unseen data** using the **test set** that we held out earlier.  \n",
    "This step evaluates the model‚Äôs ability to **generalize**, meaning it will be able to predict correctly data samples it had not seen during training; and not just memorize.\n",
    "\n",
    "#### **Why Do We Need a Test Set?**\n",
    "\n",
    "Training loss only tells us how well the model fits the data it has already seen.\n",
    "\n",
    "But a model that performs well on training data might:\n",
    "- Perform poorly on unseen data (‚ö†Ô∏è overfitting), or\n",
    "- Be too simple and fail on both (‚ö†Ô∏è underfitting)\n",
    "\n",
    "The **test loss** gives us a better picture of **real-world performance**.\n",
    "\n",
    "#### **Evaluation Steps**\n",
    "\n",
    "1. Use the final model (W, b) to predict on `X_test`\n",
    "2. Compute MSE between predicted and true test values\n",
    "3. Compare test loss to training loss\n",
    "\n",
    "#### **Interpreting the Results**\n",
    "\n",
    "| Observation                      | Diagnosis            |\n",
    "|----------------------------------|----------------------|\n",
    "| Both train and test loss high   | ‚ùå Underfitting       |\n",
    "| Train loss low, test loss high  | ‚ùå Overfitting        |\n",
    "| Both reasonably low             | ‚úÖ Generalization     |\n",
    "\n",
    "> ‚úÖ We aim for **low loss on both sets**. That‚Äôs a sign of a model that has *learned general patterns* in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(234.8185, grad_fn=<MeanBackward0>)\n",
      "Test Loss: 234.8185\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "\n",
    "# Step 1: Predict on test data\n",
    "y_test_pred = predict(X_test)\n",
    "\n",
    "# Step 2: Compute MSE on test set\n",
    "test_loss = mse_loss(y_test_pred, y_test)\n",
    "\n",
    "print(test_loss)\n",
    "\n",
    "# Step 3: Print the result\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚úÖ If the test loss is **close to the training loss**, this suggests that the model generalizes well.\n",
    "\n",
    "If the test loss is **significantly higher**, the model may have **memorized the training data** instead of learning real patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Code Task 1.2.10.1: Implement and Evaluate MAE Loss on Test Set\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Define a new function named `mae_loss(predictions, targets)` that computes the **Mean Absolute Error (MAE)** using the formula:  \n",
    "   $$\n",
    "   \\text{MAE} = \\frac{1}{n} \\sum |y_{\\text{true}} - y_{\\text{pred}}|\n",
    "   $$\n",
    "\n",
    "2. Use this function to compute the test loss from:\n",
    "   - `X_test` and `y_test` (already defined in the notebook)\n",
    "   - Use `predict(X_test)` to generate predictions\n",
    "\n",
    "3. Store the result in a variable named `test_mae_loss`\n",
    "\n",
    "4. Print the result using `test_mae_loss.item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE Loss: 12.3744\n"
     ]
    }
   ],
   "source": [
    "def mae_loss(predictions, targets):\n",
    "    return torch.mean(abs(predictions - targets))\n",
    "\n",
    "# test on X_test\n",
    "y_test_pred = predict(X_test)\n",
    "\n",
    "# calculate test loss\n",
    "test_mae_loss = mae_loss(y_test_pred, y_test)\n",
    "\n",
    "# Step 4: Print result\n",
    "print(f\"Test MAE Loss: {test_mae_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Reflection and Summary\n",
    "\n",
    "üéâ You‚Äôve just completed a full, hands-on walkthrough of **manual linear regression in PyTorch**!\n",
    "\n",
    "More than just writing code, you‚Äôve built an understanding of:\n",
    "- What a model actually *is*\n",
    "- How it learns\n",
    "- How to monitor and evaluate that learning\n",
    "\n",
    "#### **What We Built**\n",
    "\n",
    "We developed a **complete training pipeline** from scratch:\n",
    "\n",
    "- A **linear model**:‚ÄÉ$\\hat{y} = XW + b$\n",
    "- A custom **loss function**: Mean Squared Error (MSE)\n",
    "- A manual **training loop** that:\n",
    "  - Computes predictions\n",
    "  - Measures loss\n",
    "  - Calculates gradients via `.backward()`\n",
    "  - Updates parameters via gradient descent\n",
    "  - Tracks and plots loss\n",
    "\n",
    "#### **What We Learned**\n",
    "\n",
    "| Core Concept           | What You Now Know                                                 |\n",
    "|------------------------|-------------------------------------------------------------------|\n",
    "| Linear Regression      | The simplest building block of neural networks                   |\n",
    "| PyTorch Tensors        | How to represent and manipulate data, weights, and gradients     |\n",
    "| Gradient Descent       | How a model learns from loss and adjusts its parameters          |\n",
    "| Training/Test Split    | How to evaluate generalization, not just memorization            |\n",
    "| Loss Visualization     | How to monitor and debug the learning process                    |\n",
    "| Model Evaluation       | How to detect overfitting, underfitting, or generalization       |\n",
    "\n",
    "\n",
    "#### **Why This Matters**\n",
    "\n",
    "This notebook has shown you that even the most advanced deep learning models are built on **simple components like this one**.  \n",
    "A neural network without hidden layers *is* a linear model.\n",
    "\n",
    "> ‚úÖ Understanding these fundamentals gives us the clarity to build and debug much deeper architectures later.\n",
    "\n",
    "\n",
    "#### **Flow of Algorithm**\n",
    "\n",
    "```text\n",
    "Input Data (X)         Target (y)\n",
    "      ‚Üì                     ‚Üì\n",
    "Forward Pass:        ≈∑ = XW + b\n",
    "      ‚Üì\n",
    "Loss Computation:    MSE(≈∑, y)\n",
    "      ‚Üì\n",
    "Backward Pass:       .backward()\n",
    "      ‚Üì\n",
    "Gradient Descent:    W ‚Üê W - Œ± ‚ãÖ ‚àáW,   b ‚Üê b - Œ± ‚ãÖ ‚àáb\n",
    "      ‚Üì\n",
    "Repeat over Epochs ‚Üí Track & Plot Loss ‚Üí Evaluate on Test Set\n",
    "```\n",
    "#### **What‚Äôs Coming Next**\n",
    "\n",
    "This notebook introduced a model that can only draw **straight lines**.  \n",
    "But most real-world problems involve **non-linear relationships**!\n",
    "\n",
    "#### **Up Next, We Will:**\n",
    "\n",
    "- üîç Explore the **limits** of linear models  \n",
    "- ‚ö° Introduce **activation functions** like ReLU  \n",
    "- üß± Build our first **neural network with a hidden layer**\n",
    "\n",
    "‚úÖ We‚Äôre now ready to go from **lines to layers**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
